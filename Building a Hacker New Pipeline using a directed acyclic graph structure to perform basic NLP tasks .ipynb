{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building a Hacker New Pipeline to Perform Basic NLP Tasks\n",
    "\n",
    "In this project, we'll build and use a pipeline to filter, aggregate, and summarize data from [Hacker News](https://news.ycombinator.com/). Hacker News is a link aggregator website where users vote up stories that are interesting to the community. It is similar to [Reddit](https://www.reddit.com/), but the community only revolves around on computer science and entrepreneurship posts.\n",
    "\n",
    "The [JSON file](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON) we'll use, `hn_stories_2014.json`, contains a list of top voted JSON posts from 2014. The JSON file contains a single key `stories`, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
    "\n",
    "|Key|Description|\n",
    "|---|-----------|\n",
    "|created_at|A timestamp of the story's creation time.|\n",
    "|created_at_i|A unix epoch timestamp.|\n",
    "|url|The URL of the story link.|\n",
    "|objectID|The ID of the story.|\n",
    "|author|The story's author (username on HN).|\n",
    "|points|The number of upvotes the story had.|\n",
    "|title|The headline of the post.|\n",
    "|num_comments|The number of a comments a post has.|\n",
    "\n",
    "<br>\n",
    "\n",
    "Using this dataset, **we will run a sequence of basic natural language processing (NLP) tasks using our `Pipeline` class**. Our `Pipeline` structure is based on a [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph) (`DAG`).\n",
    "\n",
    "The goal will be to find the top 100 keywords of Hacker News posts in 2014. Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014!\n",
    "\n",
    "## 1. Building the `Pipeline` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries we'll use\n",
    "import json\n",
    "from datetime import datetime\n",
    "import io\n",
    "import csv\n",
    "import string\n",
    "import csv\n",
    "from collections import deque\n",
    "import itertools\n",
    "\n",
    "# Class for the directed acyclic graph (DAG) used in the Pipeline class\n",
    "class DAG:\n",
    "    \"\"\"A directed acyclic graph (DAG) data structure class.\n",
    "    \n",
    "    Attributes:\n",
    "    ----------\n",
    "        graph (dictionary): Lists nodes (keys) and \n",
    "                            the list of node(s) they point to (values), if any.\n",
    "                            \n",
    "        degrees (dictionary): Lists nodes (keys) and \n",
    "                              the number of \"edges\" that point to it (values) if any.\n",
    "                              \n",
    "    Methods:\n",
    "    -------\n",
    "        in_degrees(): Builds/calculates the self.degree attribute.\n",
    "        \n",
    "        sort(): Returns list of topologically sorted nodes using Kahn's Algorithm.\n",
    "                Essentially, it's a root node filter\n",
    "                \n",
    "        add(node, to=None): Adds a node and the node it points to (to), if any, to\n",
    "                            the self.graph attribute.\n",
    "        \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructs graph attribute for the DAG object.\"\"\"\n",
    "        self.graph = {}\n",
    "\n",
    "    def _in_degrees(self):\n",
    "        \"\"\"Constructs the degrees attribute for the DAG object.\"\"\"\n",
    "        self.degrees = {}\n",
    "        for node in self.graph:\n",
    "            if node not in self.degrees:\n",
    "                self.degrees[node] = 0\n",
    "            for pointed in self.graph[node]:\n",
    "                if pointed not in self.degrees:\n",
    "                    self.degrees[pointed] = 0\n",
    "                self.degrees[pointed] += 1\n",
    "\n",
    "    def sort(self):\n",
    "        \"\"\"Topologically sorts the degrees attribute and returns the result.\"\"\"\n",
    "        self._in_degrees()\n",
    "        root_nodes = deque()\n",
    "        for node in self.graph:\n",
    "            if self.degrees[node] == 0:\n",
    "                root_nodes.append(node)\n",
    "        \n",
    "        ordered_nodes = []\n",
    "        while root_nodes:\n",
    "            node = root_nodes.popleft()\n",
    "            for pointer in self.graph[node]:\n",
    "                self.degrees[pointer] -= 1\n",
    "                if self.degrees[pointer] == 0:\n",
    "                    root_nodes.append(pointer)\n",
    "            ordered_nodes.append(node)\n",
    "        return ordered_nodes\n",
    "\n",
    "    def add(self, node, to=None):\n",
    "        \"\"\"Adds a node and the node it points to, if any, to the graph attribute.\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "            node (immutable object): Any immutable object.\n",
    "            \n",
    "            to (immutable object): Any immutable object.\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "            list: A list of topologically sorted nodes.\n",
    "        \"\"\"\n",
    "        if node not in self.graph:\n",
    "            self.graph[node] = []\n",
    "        if to:\n",
    "            if to not in self.graph:\n",
    "                self.graph[to] = []\n",
    "            self.graph[node].append(to)\n",
    "        if len(self.sort()) != len(self.graph): # Check for cycles\n",
    "            raise Exception\n",
    "\n",
    "# Class for our Pipeline, using the DAG structure upon initialization\n",
    "class Pipeline:\n",
    "    \"\"\"A task scheduler.\n",
    "    \n",
    "    Attributes:\n",
    "    ----------\n",
    "        tasks (DAG object): A directed acyclic graph that maps task and their dependencies.\n",
    "                            The DAG object itself does this in the form of a dictionary.\n",
    "                            \n",
    "    Methods:\n",
    "    -------\n",
    "        task(depends_on=None): A decorator that adds the decorated function and its dependencies\n",
    "                               to the tasks attribute.\n",
    "                               \n",
    "        run(): Runs the topologically sorted list of functions in order of dependency in\n",
    "               a nested fashion. Returns a dictionary of functions and thier outputs \n",
    "               at each step of the way.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Constucts the tasks attribute (a directed acyclic graph) for the Pipeline obejct.\"\"\"\n",
    "        self.tasks = DAG()\n",
    "\n",
    "    def task(self, depends_on=None):\n",
    "        \"\"\"A decorator that adds the decorated task and its dependencies \n",
    "           to the tasks attribute.\n",
    "           \n",
    "        Args:\n",
    "        ----\n",
    "            depends_on (Immutable object): In most uses, it's the function which the\n",
    "                                           function being decorated depends on for input.\n",
    "        \"\"\"\n",
    "        def inner(f):\n",
    "            \"\"\"Adds the decorated function and it's dependencies to the task DAG.\"\"\"\n",
    "            self.tasks.add(f)\n",
    "            if depends_on:\n",
    "                self.tasks.add(depends_on, f)\n",
    "            return f\n",
    "        return inner\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs the topologically sorted list of functions in order of dependency in\n",
    "           a nested fashion. Returns a dictionary of functions and thier outputs \n",
    "           at each step of the way. \n",
    "           \n",
    "        Returns:\n",
    "        -------\n",
    "            dictionary:  A list of functions and thier outputs \n",
    "                         at each step of the way.    \n",
    "        \"\"\"\n",
    "        scheduled = self.tasks.sort()\n",
    "        completed = {}\n",
    "\n",
    "        for task in scheduled:\n",
    "            for node, values in self.tasks.graph.items():\n",
    "                if task in values:\n",
    "                    completed[task] = task(completed[node])\n",
    "            if task not in completed:\n",
    "                completed[task] = task()\n",
    "        return completed\n",
    "\n",
    "# Instantiate the pipeline class\n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'story_text': '',\n",
       "  'created_at': '2014-05-29T08:25:40Z',\n",
       "  'story_title': None,\n",
       "  'story_id': None,\n",
       "  'comment_text': None,\n",
       "  'created_at_i': 1401351940,\n",
       "  'url': 'https://duckduckgo.com/settings',\n",
       "  'parent_id': None,\n",
       "  'objectID': '7815290',\n",
       "  'author': 'TuxLyn',\n",
       "  'points': 1,\n",
       "  'title': 'DuckDuckGo Settings',\n",
       "  '_tags': ['story', 'author_TuxLyn', 'story_7815290'],\n",
       "  'num_comments': 0,\n",
       "  '_highlightResult': {'story_text': {'matchedWords': [],\n",
       "    'value': '',\n",
       "    'matchLevel': 'none'},\n",
       "   'author': {'matchedWords': [], 'value': 'TuxLyn', 'matchLevel': 'none'},\n",
       "   'url': {'matchedWords': [],\n",
       "    'value': 'https://duckduckgo.com/settings',\n",
       "    'matchLevel': 'none'},\n",
       "   'title': {'matchedWords': [],\n",
       "    'value': 'DuckDuckGo Settings',\n",
       "    'matchLevel': 'none'}},\n",
       "  'story_url': None},\n",
       " {'story_text': '',\n",
       "  'created_at': '2014-05-29T08:23:46Z',\n",
       "  'story_title': None,\n",
       "  'story_id': None,\n",
       "  'comment_text': None,\n",
       "  'created_at_i': 1401351826,\n",
       "  'url': 'http://bits.blogs.nytimes.com/2014/05/28/making-twitter-easier-to-use/',\n",
       "  'parent_id': None,\n",
       "  'objectID': '7815285',\n",
       "  'author': 'Leynos',\n",
       "  'points': 1,\n",
       "  'title': 'Making Twitter Easier to Use',\n",
       "  '_tags': ['story', 'author_Leynos', 'story_7815285'],\n",
       "  'num_comments': 0,\n",
       "  '_highlightResult': {'story_text': {'matchedWords': [],\n",
       "    'value': '',\n",
       "    'matchLevel': 'none'},\n",
       "   'author': {'matchedWords': [], 'value': 'Leynos', 'matchLevel': 'none'},\n",
       "   'url': {'matchedWords': [],\n",
       "    'value': 'http://bits.blogs.nytimes.com/2014/05/28/making-twitter-easier-to-use/',\n",
       "    'matchLevel': 'none'},\n",
       "   'title': {'matchedWords': [],\n",
       "    'value': 'Making Twitter Easier to Use',\n",
       "    'matchLevel': 'none'}},\n",
       "  'story_url': None}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in json file data\n",
    "def file_to_json():\n",
    "    \"\"\"Loads in JSON file as a list of dictionary objects.\"\"\"\n",
    "    with open('hn_stories_2014.json') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "    return stories\n",
    "\n",
    "# Load JSON data as a list of dictionaries and display the first 3 entries\n",
    "stories = file_to_json()\n",
    "stories[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting the most popular stories\n",
    "\n",
    "We'll  filter the list of stories to get the most popular stories of the year.\n",
    "\n",
    "Like any social link aggregator site, individual users can post whatever content they want. The reason we want the most popular stories is to ensure that we select stories that were the most talked about during the year. We can filter for popular stories by ensuring they are links (not `Ask HN` posts), have a good number of points, and have some comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'story_text': '', 'created_at': '2014-05-29T04:27:42Z', 'story_title': None, 'story_id': None, 'comment_text': None, 'created_at_i': 1401337662, 'url': 'http://krebsonsecurity.com/2014/05/true-goodbye-using-truecrypt-is-not-secure/', 'parent_id': None, 'objectID': '7814725', 'author': 'panarky', 'points': 60, 'title': 'True Goodbye: ‘Using TrueCrypt Is Not Secure’', '_tags': ['story', 'author_panarky', 'story_7814725'], 'num_comments': 23, '_highlightResult': {'story_text': {'matchedWords': [], 'value': '', 'matchLevel': 'none'}, 'author': {'matchedWords': [], 'value': 'panarky', 'matchLevel': 'none'}, 'url': {'matchedWords': [], 'value': 'http://krebsonsecurity.com/2014/05/true-goodbye-using-truecrypt-is-not-secure/', 'matchLevel': 'none'}, 'title': {'matchedWords': [], 'value': 'True Goodbye: ‘Using TrueCrypt Is Not Secure’', 'matchLevel': 'none'}}, 'story_url': None}\n",
      "{'story_text': '', 'created_at': '2014-05-29T03:51:01Z', 'story_title': None, 'story_id': None, 'comment_text': None, 'created_at_i': 1401335461, 'url': 'http://projects.aljazeera.com/2014/portrait-of-down-syndrome/index.html', 'parent_id': None, 'objectID': '7814608', 'author': 'mr_tyzic', 'points': 161, 'title': 'For Hire: Dedicated Young Man With Down Syndrome', '_tags': ['story', 'author_mr_tyzic', 'story_7814608'], 'num_comments': 27, '_highlightResult': {'story_text': {'matchedWords': [], 'value': '', 'matchLevel': 'none'}, 'author': {'matchedWords': [], 'value': 'mr_tyzic', 'matchLevel': 'none'}, 'url': {'matchedWords': [], 'value': 'http://projects.aljazeera.com/2014/portrait-of-down-syndrome/index.html', 'matchLevel': 'none'}, 'title': {'matchedWords': [], 'value': 'For Hire: Dedicated Young Man With Down Syndrome', 'matchLevel': 'none'}}, 'story_url': None}\n",
      "{'story_text': '', 'created_at': '2014-05-29T02:59:12Z', 'story_title': None, 'story_id': None, 'comment_text': None, 'created_at_i': 1401332352, 'url': 'http://www.damninteresting.com/absolute-zero-is-0k', 'parent_id': None, 'objectID': '7814466', 'author': 'jqm', 'points': 92, 'title': 'Absolute Zero', '_tags': ['story', 'author_jqm', 'story_7814466'], 'num_comments': 19, '_highlightResult': {'story_text': {'matchedWords': [], 'value': '', 'matchLevel': 'none'}, 'author': {'matchedWords': [], 'value': 'jqm', 'matchLevel': 'none'}, 'url': {'matchedWords': [], 'value': 'http://www.damninteresting.com/absolute-zero-is-0k', 'matchLevel': 'none'}, 'title': {'matchedWords': [], 'value': 'Absolute Zero', 'matchLevel': 'none'}}, 'story_url': None}\n"
     ]
    }
   ],
   "source": [
    "# The function gets the most popular stories\n",
    "def filter_stories(stories):\n",
    "    \"\"\"Filters popular stories from a list of dictionaries.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        stories (list): A list of dictionaries.\n",
    "    Returns:\n",
    "    -------\n",
    "        generator: A generator of filtered stories\n",
    "    \"\"\"\n",
    "    def is_popular(story):\n",
    "        \"\"\"A boolean filter that filters popular stories that have more than 50 points,\n",
    "           more than 1 comment, and do not begin with Ask HN.\n",
    "        \"\"\"\n",
    "        return story['num_comments'] > 1 and story['points'] > 50 and \\\n",
    "               not story['title'].startswith('Ask HN')\n",
    "    \n",
    "    return (story for story in stories if is_popular(story))\n",
    "\n",
    "# Get the most popular stories\n",
    "popular_stories = filter_stories(stories)\n",
    "\n",
    "# Display the first three entries of popular_stories\n",
    "counter = 0\n",
    "for story in popular_stories:\n",
    "    if counter < 3:\n",
    "        print(story)\n",
    "        counter += 1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "popular_stories = filter_stories(stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Writing popular stories to a CSV file\n",
    "\n",
    "With a reduced set of stories, it's time to write these `dict` objects to a CSV file. The purpose of translating the dictionaries to a CSV is that we want to have a consistent data format when running the later summarizations. By keeping consistent data formats, each of our pipeline tasks will be adaptable with future task requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for writing data into a csv file\n",
    "def build_csv(lines, header=None, file=None):\n",
    "    \"\"\"Writes lines into a csv file.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        lines (iterable): A generator or list or dictionary keys.\n",
    "        \n",
    "        header (list): A list of string names equal in length to lines\n",
    "        \n",
    "        file (string): A file name.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        string: The file name.\n",
    "        \n",
    "    \"\"\"\n",
    "    if header:\n",
    "        lines = itertools.chain([header], lines)\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.StringIO object at 0x7f128bc1fdc0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function saves the JSON data to a csv file\n",
    "def json_to_csv(stories):\n",
    "    \"\"\"Saves JSON data to a csv file.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        stories (generator): A list of dictionaries containing the most popular stories.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "        file object: <The file object at x location>\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append((story['objectID'], \n",
    "                     datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"), \n",
    "                     story['url'], \n",
    "                     story['points'], \n",
    "                     story['title']))\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'],\n",
    "                     file=io.StringIO())\n",
    "\n",
    "csv_file = json_to_csv(popular_stories)\n",
    "print(csv_file)\n",
    "csv_file.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extracting story titles\n",
    "\n",
    "Using the CSV file format we created in the previous task, we can now extract the title column. Once we have extracted the titles of each popular post, we can then run the next word frequency task.\n",
    "\n",
    "We'll do the following steps to extract titles:\n",
    "1. Create a `csv.reader()` object from the file object. \n",
    "2. Find the index of the `title` in the header. \n",
    "3. Iterate the through the reader, and return each item from the reader in the corresponding title index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Goodbye: ‘Using TrueCrypt Is Not Secure’\n",
      "For Hire: Dedicated Young Man With Down Syndrome\n",
      "Absolute Zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function extracts the titles from the JSON data turned CSV file.\n",
    "def extract_titles(csv_file):\n",
    "    \"\"\"Extracts titles from CSV file.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        csv_file (string): The name of the CSV file we want to extract titles from.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        generator: A list of titles in string format.\n",
    "    \"\"\"\n",
    "    if not isinstance(csv_file, str):\n",
    "            csv_file.seek(0)\n",
    "            reader = csv.reader(csv_file)\n",
    "            header = next(reader)\n",
    "            idx = header.index('title')\n",
    "            return (line[idx] for line in reader)\n",
    "    else:\n",
    "        with open(csv_file) as file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            header = next(reader)\n",
    "            idx = header.index('title')\n",
    "            return (line[idx] for line in reader)\n",
    "        \n",
    "# Extract titles from CSV file\n",
    "titles = extract_titles(csv_file)\n",
    "\n",
    "# Display the first three entries of titles\n",
    "counter = 0\n",
    "for title in titles:\n",
    "    if counter < 3:\n",
    "        print(title)\n",
    "        counter += 1   \n",
    "        \n",
    "# Reset iteration on csv file       \n",
    "csv_file.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Cleaning title\n",
    "\n",
    "Because we're trying to create a word frequency model of words from Hacker News titles, we need a way to create a consistent set of words to use. For example, words like `Google`, `google`, `GooGle?`, and `google.`, all mean the same keyword: `google`. If we were to split the title into words, however, they would all be lumped into different categories.\n",
    "\n",
    "To clean the titles, we should make sure to lower case the titles, and to remove the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true goodbye ‘using truecrypt is not secure’\n",
      "for hire dedicated young man with down syndrome\n",
      "absolute zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function converts all titles to lowercase and removes any punctuation.\n",
    "def clean_titles(titles):\n",
    "    \"\"\"Cleans titles by converting each to lowercase and removing puncuation.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        titles (generator): A list of titles.\n",
    "    \n",
    "    Yields:\n",
    "    ------\n",
    "        string: The title with lowercase letters and punctuation removed.\"\"\"\n",
    "    return (''.join(c for c in title.lower() if c not in string.punctuation) for title in titles)\n",
    "        \n",
    "# Clean titles\n",
    "clean_titles = clean_titles(extract_titles(csv_file))\n",
    "\n",
    "# Display the first three entries of clean_titles\n",
    "counter = 0\n",
    "for title in clean_titles:\n",
    "    if counter < 3:\n",
    "        print(title)\n",
    "        counter += 1   \n",
    "        \n",
    "# Reset iteration on csv file       \n",
    "csv_file.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generating a word frequency dictionary\n",
    "\n",
    "With a cleaned title, we can now build the **word frequency** dictionary. A word frequency dictionary are key value pairs that connects a word to the number of times it is used in a text.\n",
    "\n",
    "To find actual keywords, we should enforce the word frequency dictionary to not include **stop words**. Stop words are words that occur frequently in language like \"the\", \"or\", etc., and are commonly rejected in keyword searches.\n",
    "\n",
    "## 6.1 Stop words list\n",
    "\n",
    "A list of stop words was copied and pasted from [countwordsfree.com](https://countwordsfree.com/stopwords). A few additional stop words were added to the end for the purposes of our keyword search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasted stop words converted into a tuple\n",
    "stop_words = tuple(\"\"\"able\n",
    "about\n",
    "above\n",
    "abroad\n",
    "according\n",
    "accordingly\n",
    "across\n",
    "actually\n",
    "adj\n",
    "after\n",
    "afterwards\n",
    "again\n",
    "against\n",
    "ago\n",
    "ahead\n",
    "ain't\n",
    "all\n",
    "allow\n",
    "allows\n",
    "almost\n",
    "alone\n",
    "along\n",
    "alongside\n",
    "already\n",
    "also\n",
    "although\n",
    "always\n",
    "am\n",
    "amid\n",
    "amidst\n",
    "among\n",
    "amongst\n",
    "an\n",
    "and\n",
    "another\n",
    "any\n",
    "anybody\n",
    "anyhow\n",
    "anyone\n",
    "anything\n",
    "anyway\n",
    "anyways\n",
    "anywhere\n",
    "apart\n",
    "appear\n",
    "appreciate\n",
    "appropriate\n",
    "are\n",
    "aren't\n",
    "around\n",
    "as\n",
    "a's\n",
    "aside\n",
    "ask\n",
    "asking\n",
    "associated\n",
    "at\n",
    "available\n",
    "away\n",
    "awfully\n",
    "back\n",
    "backward\n",
    "backwards\n",
    "be\n",
    "became\n",
    "because\n",
    "become\n",
    "becomes\n",
    "becoming\n",
    "been\n",
    "before\n",
    "beforehand\n",
    "begin\n",
    "behind\n",
    "being\n",
    "believe\n",
    "below\n",
    "beside\n",
    "besides\n",
    "best\n",
    "better\n",
    "between\n",
    "beyond\n",
    "both\n",
    "brief\n",
    "but\n",
    "by\n",
    "came\n",
    "can\n",
    "cannot\n",
    "cant\n",
    "can't\n",
    "caption\n",
    "cause\n",
    "causes\n",
    "certain\n",
    "certainly\n",
    "changes\n",
    "clearly\n",
    "c'mon\n",
    "co\n",
    "co.\n",
    "com\n",
    "come\n",
    "comes\n",
    "concerning\n",
    "consequently\n",
    "consider\n",
    "considering\n",
    "contain\n",
    "containing\n",
    "contains\n",
    "corresponding\n",
    "could\n",
    "couldn't\n",
    "course\n",
    "c's\n",
    "currently\n",
    "dare\n",
    "daren't\n",
    "definitely\n",
    "described\n",
    "despite\n",
    "did\n",
    "didn't\n",
    "different\n",
    "directly\n",
    "do\n",
    "does\n",
    "doesn't\n",
    "doing\n",
    "done\n",
    "don't\n",
    "down\n",
    "downwards\n",
    "during\n",
    "each\n",
    "edu\n",
    "eg\n",
    "eight\n",
    "eighty\n",
    "either\n",
    "else\n",
    "elsewhere\n",
    "end\n",
    "ending\n",
    "enough\n",
    "entirely\n",
    "especially\n",
    "et\n",
    "etc\n",
    "even\n",
    "ever\n",
    "evermore\n",
    "every\n",
    "everybody\n",
    "everyone\n",
    "everything\n",
    "everywhere\n",
    "ex\n",
    "exactly\n",
    "example\n",
    "except\n",
    "fairly\n",
    "far\n",
    "farther\n",
    "few\n",
    "fewer\n",
    "fifth\n",
    "first\n",
    "five\n",
    "followed\n",
    "following\n",
    "follows\n",
    "for\n",
    "forever\n",
    "former\n",
    "formerly\n",
    "forth\n",
    "forward\n",
    "found\n",
    "four\n",
    "from\n",
    "further\n",
    "furthermore\n",
    "get\n",
    "gets\n",
    "getting\n",
    "given\n",
    "gives\n",
    "go\n",
    "goes\n",
    "going\n",
    "gone\n",
    "got\n",
    "gotten\n",
    "greetings\n",
    "had\n",
    "hadn't\n",
    "half\n",
    "happens\n",
    "hardly\n",
    "has\n",
    "hasn't\n",
    "have\n",
    "haven't\n",
    "having\n",
    "he\n",
    "he'd\n",
    "he'll\n",
    "hello\n",
    "help\n",
    "hence\n",
    "her\n",
    "here\n",
    "hereafter\n",
    "hereby\n",
    "herein\n",
    "here's\n",
    "hereupon\n",
    "hers\n",
    "herself\n",
    "he's\n",
    "hi\n",
    "him\n",
    "himself\n",
    "his\n",
    "hither\n",
    "hopefully\n",
    "how\n",
    "howbeit\n",
    "however\n",
    "hundred\n",
    "i'd\n",
    "ie\n",
    "if\n",
    "ignored\n",
    "i'll\n",
    "i'm\n",
    "immediate\n",
    "in\n",
    "inasmuch\n",
    "inc\n",
    "inc.\n",
    "indeed\n",
    "indicate\n",
    "indicated\n",
    "indicates\n",
    "inner\n",
    "inside\n",
    "insofar\n",
    "instead\n",
    "into\n",
    "inward\n",
    "is\n",
    "isn't\n",
    "it\n",
    "it'd\n",
    "it'll\n",
    "its\n",
    "it's\n",
    "itself\n",
    "i've\n",
    "just\n",
    "k\n",
    "keep\n",
    "keeps\n",
    "kept\n",
    "know\n",
    "known\n",
    "knows\n",
    "last\n",
    "lately\n",
    "later\n",
    "latter\n",
    "latterly\n",
    "least\n",
    "less\n",
    "lest\n",
    "let\n",
    "let's\n",
    "like\n",
    "liked\n",
    "likely\n",
    "likewise\n",
    "little\n",
    "look\n",
    "looking\n",
    "looks\n",
    "low\n",
    "lower\n",
    "ltd\n",
    "made\n",
    "mainly\n",
    "make\n",
    "makes\n",
    "many\n",
    "may\n",
    "maybe\n",
    "mayn't\n",
    "me\n",
    "mean\n",
    "meantime\n",
    "meanwhile\n",
    "merely\n",
    "might\n",
    "mightn't\n",
    "mine\n",
    "minus\n",
    "miss\n",
    "more\n",
    "moreover\n",
    "most\n",
    "mostly\n",
    "mr\n",
    "mrs\n",
    "much\n",
    "must\n",
    "mustn't\n",
    "my\n",
    "myself\n",
    "name\n",
    "namely\n",
    "nd\n",
    "near\n",
    "nearly\n",
    "necessary\n",
    "need\n",
    "needn't\n",
    "needs\n",
    "neither\n",
    "never\n",
    "neverf\n",
    "neverless\n",
    "nevertheless\n",
    "new\n",
    "next\n",
    "nine\n",
    "ninety\n",
    "no\n",
    "nobody\n",
    "non\n",
    "none\n",
    "nonetheless\n",
    "noone\n",
    "no-one\n",
    "nor\n",
    "normally\n",
    "not\n",
    "nothing\n",
    "notwithstanding\n",
    "novel\n",
    "now\n",
    "nowhere\n",
    "obviously\n",
    "of\n",
    "off\n",
    "often\n",
    "oh\n",
    "ok\n",
    "okay\n",
    "old\n",
    "on\n",
    "once\n",
    "one\n",
    "ones\n",
    "one's\n",
    "only\n",
    "onto\n",
    "opposite\n",
    "or\n",
    "other\n",
    "others\n",
    "otherwise\n",
    "ought\n",
    "oughtn't\n",
    "our\n",
    "ours\n",
    "ourselves\n",
    "out\n",
    "outside\n",
    "over\n",
    "overall\n",
    "own\n",
    "particular\n",
    "particularly\n",
    "past\n",
    "per\n",
    "perhaps\n",
    "placed\n",
    "please\n",
    "plus\n",
    "possible\n",
    "presumably\n",
    "probably\n",
    "provided\n",
    "provides\n",
    "que\n",
    "quite\n",
    "qv\n",
    "rather\n",
    "rd\n",
    "re\n",
    "really\n",
    "reasonably\n",
    "recent\n",
    "recently\n",
    "regarding\n",
    "regardless\n",
    "regards\n",
    "relatively\n",
    "respectively\n",
    "right\n",
    "round\n",
    "said\n",
    "same\n",
    "saw\n",
    "say\n",
    "saying\n",
    "says\n",
    "second\n",
    "secondly\n",
    "see\n",
    "seeing\n",
    "seem\n",
    "seemed\n",
    "seeming\n",
    "seems\n",
    "seen\n",
    "self\n",
    "selves\n",
    "sensible\n",
    "sent\n",
    "serious\n",
    "seriously\n",
    "seven\n",
    "several\n",
    "shall\n",
    "shan't\n",
    "she\n",
    "she'd\n",
    "she'll\n",
    "she's\n",
    "should\n",
    "shouldn't\n",
    "since\n",
    "six\n",
    "so\n",
    "some\n",
    "somebody\n",
    "someday\n",
    "somehow\n",
    "someone\n",
    "something\n",
    "sometime\n",
    "sometimes\n",
    "somewhat\n",
    "somewhere\n",
    "soon\n",
    "sorry\n",
    "specified\n",
    "specify\n",
    "specifying\n",
    "still\n",
    "sub\n",
    "such\n",
    "sup\n",
    "sure\n",
    "take\n",
    "taken\n",
    "taking\n",
    "tell\n",
    "tends\n",
    "th\n",
    "than\n",
    "thank\n",
    "thanks\n",
    "thanx\n",
    "that\n",
    "that'll\n",
    "thats\n",
    "that's\n",
    "that've\n",
    "the\n",
    "their\n",
    "theirs\n",
    "them\n",
    "themselves\n",
    "then\n",
    "thence\n",
    "there\n",
    "thereafter\n",
    "thereby\n",
    "there'd\n",
    "therefore\n",
    "therein\n",
    "there'll\n",
    "there're\n",
    "theres\n",
    "there's\n",
    "thereupon\n",
    "there've\n",
    "these\n",
    "they\n",
    "they'd\n",
    "they'll\n",
    "they're\n",
    "they've\n",
    "thing\n",
    "things\n",
    "think\n",
    "third\n",
    "thirty\n",
    "this\n",
    "thorough\n",
    "thoroughly\n",
    "those\n",
    "though\n",
    "three\n",
    "through\n",
    "throughout\n",
    "thru\n",
    "thus\n",
    "till\n",
    "to\n",
    "together\n",
    "too\n",
    "took\n",
    "toward\n",
    "towards\n",
    "tried\n",
    "tries\n",
    "truly\n",
    "try\n",
    "trying\n",
    "t's\n",
    "twice\n",
    "two\n",
    "un\n",
    "under\n",
    "underneath\n",
    "undoing\n",
    "unfortunately\n",
    "unless\n",
    "unlike\n",
    "unlikely\n",
    "until\n",
    "unto\n",
    "up\n",
    "upon\n",
    "upwards\n",
    "us\n",
    "use\n",
    "used\n",
    "useful\n",
    "uses\n",
    "using\n",
    "usually\n",
    "v\n",
    "value\n",
    "various\n",
    "versus\n",
    "very\n",
    "via\n",
    "viz\n",
    "vs\n",
    "want\n",
    "wants\n",
    "was\n",
    "wasn't\n",
    "way\n",
    "we\n",
    "we'd\n",
    "welcome\n",
    "well\n",
    "we'll\n",
    "went\n",
    "were\n",
    "we're\n",
    "weren't\n",
    "we've\n",
    "what\n",
    "whatever\n",
    "what'll\n",
    "what's\n",
    "what've\n",
    "when\n",
    "whence\n",
    "whenever\n",
    "where\n",
    "whereafter\n",
    "whereas\n",
    "whereby\n",
    "wherein\n",
    "where's\n",
    "whereupon\n",
    "wherever\n",
    "whether\n",
    "which\n",
    "whichever\n",
    "while\n",
    "whilst\n",
    "whither\n",
    "who\n",
    "who'd\n",
    "whoever\n",
    "whole\n",
    "who'll\n",
    "whom\n",
    "whomever\n",
    "who's\n",
    "whose\n",
    "why\n",
    "will\n",
    "willing\n",
    "wish\n",
    "with\n",
    "within\n",
    "without\n",
    "wonder\n",
    "won't\n",
    "would\n",
    "wouldn't\n",
    "yes\n",
    "yet\n",
    "you\n",
    "you'd\n",
    "you'll\n",
    "your\n",
    "you're\n",
    "yours\n",
    "yourself\n",
    "yourselves\n",
    "you've\n",
    "zero\n",
    "a\n",
    "how's\n",
    "i\n",
    "when's\n",
    "why's\n",
    "b\n",
    "c\n",
    "d\n",
    "e\n",
    "f\n",
    "g\n",
    "h\n",
    "j\n",
    "l\n",
    "m\n",
    "n\n",
    "o\n",
    "p\n",
    "q\n",
    "r\n",
    "s\n",
    "t\n",
    "u\n",
    "uucp\n",
    "w\n",
    "x\n",
    "y\n",
    "z\n",
    "I\n",
    "www\n",
    "amount\n",
    "bill\n",
    "bottom\n",
    "call\n",
    "computer\n",
    "con\n",
    "couldnt\n",
    "cry\n",
    "de\n",
    "describe\n",
    "detail\n",
    "due\n",
    "eleven\n",
    "empty\n",
    "fifteen\n",
    "fifty\n",
    "fill\n",
    "find\n",
    "fire\n",
    "forty\n",
    "front\n",
    "full\n",
    "give\n",
    "hasnt\n",
    "herse\n",
    "himse\n",
    "interest\n",
    "itse”\n",
    "mill\n",
    "move\n",
    "myse”\n",
    "part\n",
    "put\n",
    "show\n",
    "side\n",
    "sincere\n",
    "sixty\n",
    "system\n",
    "ten\n",
    "thick\n",
    "thin\n",
    "top\n",
    "twelve\n",
    "twenty\n",
    "abst\n",
    "accordance\n",
    "act\n",
    "added\n",
    "adopted\n",
    "affected\n",
    "affecting\n",
    "affects\n",
    "ah\n",
    "announce\n",
    "anymore\n",
    "apparently\n",
    "approximately\n",
    "aren\n",
    "arent\n",
    "arise\n",
    "auth\n",
    "beginning\n",
    "beginnings\n",
    "begins\n",
    "biol\n",
    "briefly\n",
    "ca\n",
    "date\n",
    "ed\n",
    "effect\n",
    "et-al\n",
    "ff\n",
    "fix\n",
    "gave\n",
    "giving\n",
    "heres\n",
    "hes\n",
    "hid\n",
    "home\n",
    "id\n",
    "im\n",
    "immediately\n",
    "importance\n",
    "important\n",
    "index\n",
    "information\n",
    "invention\n",
    "itd\n",
    "keys\n",
    "kg\n",
    "km\n",
    "largely\n",
    "lets\n",
    "line\n",
    "'ll\n",
    "means\n",
    "mg\n",
    "million\n",
    "ml\n",
    "mug\n",
    "na\n",
    "nay\n",
    "necessarily\n",
    "nos\n",
    "noted\n",
    "obtain\n",
    "obtained\n",
    "omitted\n",
    "ord\n",
    "owing\n",
    "page\n",
    "pages\n",
    "poorly\n",
    "possibly\n",
    "potentially\n",
    "pp\n",
    "predominantly\n",
    "present\n",
    "previously\n",
    "primarily\n",
    "promptly\n",
    "proud\n",
    "quickly\n",
    "ran\n",
    "readily\n",
    "ref\n",
    "refs\n",
    "related\n",
    "research\n",
    "resulted\n",
    "resulting\n",
    "results\n",
    "run\n",
    "sec\n",
    "section\n",
    "shed\n",
    "shes\n",
    "showed\n",
    "shown\n",
    "showns\n",
    "shows\n",
    "significant\n",
    "significantly\n",
    "similar\n",
    "similarly\n",
    "slightly\n",
    "somethan\n",
    "specifically\n",
    "state\n",
    "states\n",
    "stop\n",
    "strongly\n",
    "substantially\n",
    "successfully\n",
    "sufficiently\n",
    "suggest\n",
    "thered\n",
    "thereof\n",
    "therere\n",
    "thereto\n",
    "theyd\n",
    "theyre\n",
    "thou\n",
    "thoughh\n",
    "thousand\n",
    "throug\n",
    "til\n",
    "tip\n",
    "ts\n",
    "ups\n",
    "usefully\n",
    "usefulness\n",
    "'ve\n",
    "vol\n",
    "vols\n",
    "wed\n",
    "whats\n",
    "wheres\n",
    "whim\n",
    "whod\n",
    "whos\n",
    "widely\n",
    "words\n",
    "world\n",
    "youve\n",
    "youd\n",
    "youre\n",
    "–\n",
    "—\n",
    "hn\"\"\".split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Building a keyword dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('true', 6), ('goodbye', 10), ('‘using', 2), ('truecrypt', 5), ('secure’', 2), ('hire', 13), ('dedicated', 7), ('young', 10), ('man', 25), ('syndrome', 4), ('absolute', 2), ('joshua', 2), ('norton', 2), ('emperor', 2), ('united', 7), ('soylent', 5), ('revolution', 6), ('pleasurable', 2), ('git', 38), ('20', 24)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function builds a word frequency dictionary\n",
    "def build_keyword_dictionary(titles):\n",
    "    \"\"\"Counts word frequency within titles that are not found in a list of stop words.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        titles (generator): A List of titles in string format.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        Dictionary: A list of words (keys) and their frequencies (values).\n",
    "    \"\"\"\n",
    "    keyword_count = {}\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stop_words:\n",
    "                if word not in keyword_count:\n",
    "                    keyword_count[word] = 1\n",
    "                keyword_count[word] += 1\n",
    "    return keyword_count\n",
    "\n",
    "# Build word frequency dictionary\n",
    "word_frequency = build_keyword_dictionary(clean_titles(extract_titles(csv_file)))\n",
    "\n",
    "# Display 20 entries in word_frequency\n",
    "print(list(word_frequency.items())[:20])\n",
    "\n",
    "# Reset iteration on csv file       \n",
    "csv_file.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Sorting the word frequency dictionary\n",
    "\n",
    "Finally, we're ready to sort the top words used in all the titles.\n",
    "\n",
    "The goal is to output a list of tuples with (`word`, `frequency`) as the entries sorted from most used, to least most used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('facebook', 72)]\n"
     ]
    }
   ],
   "source": [
    "# The function sorts and returns the top 100 most frequent words\n",
    "def sort_keywords(keywords):\n",
    "    \"\"\"Generates a list of the top 100 most frequent words by sorting a \n",
    "       word frequency dictionary by value in decreasing order.\n",
    "       Displays the top 100 tuples (word, frequency).\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        keywords (dictionary): A word frequency dictionary.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        list: A list of the top 100 tuples (word, frequency).\n",
    "        \"\"\"\n",
    "    return sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "# Sort word frequency from highest to lowest\n",
    "sorted_word_freq = sort_keywords(word_frequency)\n",
    "\n",
    "# Print the top 10 most frequent words\n",
    "print(sorted_word_freq[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Putting the pipeline together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('facebook', 72), ('released', 72), ('2013', 66), ('javascript', 66), ('free', 65), ('source', 65), ('game', 64), ('internet', 63), ('microsoft', 60), ('linux', 59), ('app', 58), ('pdf', 56), ('work', 55), ('language', 55), ('software', 53), ('2014', 53), ('startup', 52), ('apple', 51), ('time', 49), ('yc', 49), ('security', 49), ('nsa', 46), ('github', 46), ('windows', 45), ('1', 41), ('project', 41), ('heartbleed', 41), ('git', 38), ('users', 38), ('dont', 38), ('design', 38), ('ios', 38), ('developer', 37), ('os', 37), ('twitter', 37), ('ceo', 37), ('life', 37), ('big', 36), ('day', 36), ('android', 35), ('online', 35), ('years', 34), ('simple', 34), ('court', 34), ('guide', 33), ('learning', 33), ('mt', 33), ('api', 33), ('apps', 33), ('browser', 33), ('server', 32), ('firefox', 32), ('fast', 32), ('gox', 32), ('problem', 32), ('mozilla', 32), ('engine', 32), ('site', 32), ('introducing', 31), ('amazon', 31), ('year', 31), ('support', 30), ('built', 30), ('people', 30), ('text', 30), ('3', 29), ('tech', 29), ('development', 29), ('billion', 28), ('developers', 28), ('library', 28), ('website', 28), ('money', 28), ('chrome', 28), ('2048', 28), ('release', 27), ('hacker', 27), ('2012', 27), ('story', 27), ('good', 27), ('silicon', 27), ('10', 26), ('haskell', 26), ('mobile', 26), ('science', 26), ('future', 26), ('learn', 26), ('public', 26), ('email', 26), ('service', 26), ('valley', 26)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline object\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Save function to our pipeline to load in json file data\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    \"\"\"Loads in JSON file as a list of dictionary objects.\"\"\"\n",
    "    with open('hn_stories_2014.json') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "    return stories\n",
    "\n",
    "# Save function which depends on the output from file_to_json\n",
    "# to our pipeline. The function gets the most popular stories\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    \"\"\"Filters popular stories from a list of dictionaries.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        stories (list): A list of dictionaries.\n",
    "    Returns:\n",
    "    -------\n",
    "        generator: A generator of filtered stories\n",
    "    \"\"\"\n",
    "    def is_popular(story):\n",
    "        \"\"\"A boolean filter that filters popular stories that have more than 50 points,\n",
    "           more than 1 comment, and do not begin with Ask HN.\n",
    "        \"\"\"\n",
    "        return story['num_comments'] > 1 and story['points'] > 50 and \\\n",
    "               not story['title'].startswith('Ask HN')\n",
    "    \n",
    "    return (story for story in stories if is_popular(story))\n",
    "\n",
    "# Save function which depends on the output from filter_stories\n",
    "# to our pipeline. The function saves the JSON data to a csv file\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    \"\"\"Saves JSON data to a csv file.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        stories (generator): A list of dictionaries containing the most popular stories.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "        file object: <The file object at x location>\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append((story['objectID'], \n",
    "                     datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"), \n",
    "                     story['url'], \n",
    "                     story['points'], \n",
    "                     story['title']))\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'],\n",
    "                     file=io.StringIO())\n",
    "\n",
    "# Save function which depends on the output from json_to_csv\n",
    "# to our pipeline. The function extracts the titles from the JSON data turned CSV file.\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    \"\"\"Extracts titles from CSV file.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        csv_file (string): The name of the CSV file we want to extract titles from.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        generator: A list of titles in string format.\n",
    "    \"\"\"\n",
    "    if not isinstance(csv_file, str):\n",
    "            csv_file.seek(0)\n",
    "            reader = csv.reader(csv_file)\n",
    "            header = next(reader)\n",
    "            idx = header.index('title')\n",
    "            return (line[idx] for line in reader)\n",
    "    else:\n",
    "        with open(csv_file) as file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            header = next(reader)\n",
    "            idx = header.index('title')\n",
    "            return (line[idx] for line in reader)\n",
    "        \n",
    "# Save function which depends on the output from extract_titles\n",
    "# to our pipeline. The function converts all titles to lowercase and removes any punctuation.\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    \"\"\"Cleans titles by converting each to lowercase and removing puncuation.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        titles (generator): A list of titles.\n",
    "    \n",
    "    Yields:\n",
    "    ------\n",
    "        string: The title with lowercase letters and punctuation removed.\"\"\"\n",
    "    return (''.join(c for c in title.lower() if c not in string.punctuation) for title in titles)\n",
    "\n",
    "# Save function which depends on the output from clean_titles\n",
    "# to our pipeline. The function builds a word frequency dictionary\n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keyword_dictionary(titles):\n",
    "    \"\"\"Counts word frequency within titles that are not found in a list of stop words.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        titles (generator): A List of titles in string format.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        Dictionary: A list of words (keys) and their frequencies (values).\n",
    "    \"\"\"\n",
    "    keyword_count = {}\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stop_words:\n",
    "                if word not in keyword_count:\n",
    "                    keyword_count[word] = 1\n",
    "                keyword_count[word] += 1\n",
    "    return keyword_count\n",
    "\n",
    "# Save function which depends on the output from build_keyword_dictionary\n",
    "# to our pipeline. The function sorts and returns the topp 100 most frequent words\n",
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def sort_keywords(keywords):\n",
    "    \"\"\"Generates a list of the top 100 most frequent words by sorting a \n",
    "       word frequency dictionary by value in decreasing order.\n",
    "       Displays the top 100 tuples (word, frequency).\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        keywords (dictionary): A word frequency dictionary.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        list: A list of the top 100 tuples (word, frequency).\n",
    "        \"\"\"\n",
    "    return sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "# Run the pipeline, saving outputs at every step in a dictionary\n",
    "pipeline_outputs = pipeline.run()\n",
    "\n",
    "# Top 100 most frequent words\n",
    "top_100_words = pipeline_outputs[sort_keywords]\n",
    "print(top_100_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The final result yielded some interesting keywords. There were terms like `bitcoin` (the cryptocurrency), `heartbleed` (the 2014 hack), and many others. Even though this was a basic natural language processing task, it did provide some interesting insights into conversations from 2014.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "* Rewrite the `Pipeline` class' output to save a file of the output for each task. This will allow us to \"checkpoint\" tasks so they don't have to be run twice.\n",
    "* Use the [`nltk` package](http://www.nltk.org/) for more advanced natural language processing tasks.\n",
    "* Convert to a CSV before filtering, so you can keep all the stories from 2014 in a raw file.\n",
    "* Fetch the data from Hacker News directly from [their JSON API](https://hn.algolia.com/api). Instead of reading from an older file, we can perform additional data processing using newer data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
